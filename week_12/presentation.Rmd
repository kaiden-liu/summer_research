---
title: "Group Meeting Presentation"
author: "Kaiden Liu"
date: "28/07/2021"
output:
  beamer_presentation:
    colortheme: "crane"
    fonttheme: "Default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

```{r, include=F}
library(BARBI)
library(tidyverse)
library(phyloseq)
library(DESeq2)
# library(R.utils)
# library(BiocParallel)
# library(doParallel)
# library(parallel)
library(HDInterval)
# library(grid)
# library(xtable)
# library(gtable)
# library(gridExtra)
# library(BiocStyle)
library(ggrepel)# library(ggrepel)
library(dplyr)
#R.utils::sourceDirectory("C:/Users/kaide/Documents/Summer_Research/week/week 11/R_BARBI_CJS")
```


```{r, include=F}
ps <- readRDS("C:/Users/kaide/Documents/Summer_Research/week/week 11/Data/ps_zymo.rds")
if(dim(otu_table(ps))[1]!=ntaxa(ps)){otu_table(ps) <- t(otu_table(ps))}
```



```{r, include=F}
ncont <- paste0("NegativeControl.",seq(1,10))
stan <- paste0("Standard.Dilution.1.",c(1,6,36,216,1296,7776,46656,279936))

sample_data(ps)$Name <- factor(sample_data(ps)$Name, levels = c(ncont,stan))

sample_names(ps) <- as.character(sample_data(ps)$Name)
```


```{r, include=F}
ASV <- as.character(paste0("ASV_",seq(1,ntaxa(ps))))
ASV.Genus <- paste0("ASV_",seq(1,ntaxa(ps)),"_",as.character(tax_table(ps)[,6]))
ASV.Genus.Species <- paste0(ASV,"_",as.character(tax_table(ps)[,6]),"_", as.character(tax_table(ps)[,7]))

df.ASV <- data.frame(seq.variant = taxa_names(ps), ASV = ASV, ASV.Genus = ASV.Genus, ASV.Genus.Species = ASV.Genus.Species)
```


```{r, include= F}
taxa_names(ps) <- df.ASV$ASV.Genus.Species
```




```{r adding_blocks, include=F}
blocks <- rep("Set1", nsamples(ps))

sample_data(ps)$block <- blocks
```


```{r filter_taxa, include=F}
ps <- prune_taxa(taxa_sums(ps) > 0, ps)
ps.standard <- subset_samples(ps, SampleType %in% c("Standard"))
prevTaxaP <- apply(otu_table(ps.standard), 1, function(x){sum(x>0)})

Contaminants1 <- names(prevTaxaP)[prevTaxaP == 0]
length(Contaminants1)
ps <- prune_taxa(prevTaxaP > 0, ps)
ps
```


```{r summary_stat, include=F}
table(sample_data(ps)$SampleType, sample_data(ps)$block)
colSums(otu_table(ps))
```


```{r list_of_phyloseq, include=F}
psBlockResult <- psBlockResults(ps, sampleTypeVar = "SampleType", caselevels = c("Standard"), controllevel="Negative", sampleName = "Name", blockVar = "block")

psByBlock <- psBlockResult[[1]]
psNCbyBlock <- psBlockResult[[2]] #contaminant intensity
psallzeroInNC <- psBlockResult[[3]] # zeros in negative control
psPlByBlock <- psBlockResult[[4]] # biological smaples

test<-lapply(psNCbyBlock, function(x) {
ps.to.dq = phyloseq_to_deseq2(x, design = ~1)
ps.to.dq = estimateSizeFactors(ps.to.dq,type="poscounts")
return(ps.to.dq)
}

)
test

```


```{r estimate_Cont_ncontrols, include=F}
alphaBetaNegControl <- alphaBetaNegControl(psNCbyBlock = psNCbyBlock, stringent = FALSE)
```

```{r estimate_Cont_plasma, include=F}
num_blks <- length(alphaBetaNegControl)
blks <- seq(1, num_blks) %>% as.list

gammaPrior_all_blks <- lapply(blks, function(x){
        gammaPrior <- alphaBetaContInPlasma(psPlByBlock = psPlByBlock, psallzeroInNC = psallzeroInNC, blk = x, alphaBetaNegControl = alphaBetaNegControl)
        return(gammaPrior)
})

```



```{r, include=F}
# First sample, second taxon
sam <- 1
taxa <- 2

# prior density for contamination intensity in biological samples
# we have only one batch
gammaPrior_Cont <- gammaPrior_all_blks[[1]]

# observed count in sam == 1 and second taxa
k <- as.numeric(gammaPrior_Cont[[sam]]$kij[taxa])
al_c <- gammaPrior_Cont[[sam]]$alpha_ij_c[taxa]
be_c <- gammaPrior_Cont[[sam]]$beta_ij_c[taxa]


# true intensity grid (alternatively we can choose min and max of k as well)
maxu <- max(rgamma(n = 1000,shape = al_c, rate = be_c))
# include minu
u <- seq(0, maxu, by = .1)

post_lr <- posterior(u,k,al_c,be_c)
# post_lr <- lapply(as.list(u), function(j){posterior(j, k, al_c, be_c )}) %>% unlist()

# plot(u, post_lr)
#######

lc <- rgamma(length(u), shape = al_c, rate = be_c)
# kij_c_lc <- lapply(as.list(lc), function(x){rpois(1, x)}) %>% unlist()
# 95% credible interval for lc
c(qgamma(.025, shape = al_c, rate = be_c), 
  qgamma(.925, shape = al_c, rate = be_c))


hist(lc)

#######

start.time<-Sys.time()
chain <- MH_MCMC(itera = 100,
                            k = as.numeric(gammaPrior_Cont[[sam]]$kij[taxa]),
                            al_c = gammaPrior_Cont[[sam]]$alpha_ij_c[taxa],
                            be_c = gammaPrior_Cont[[sam]]$beta_ij_c[taxa],
                            startvalue_lamda_r = 0)

end.time<-Sys.time()
time.taken_mh<- end.time-start.time

lr_star <- chain[50:100]
quantile(lr_star, c(0.025, 0.975))

hist(lr_star)

mean(lr_star)
mean(lc)

## generate the data 
kij_r_star <- rpois(1, mean(lr_star))
kij_c_star <- rpois(1, mean(lc))

kij_r_star+kij_c_star # is it closer to as.numeric(gammaPrior_Cont[[sam]]$kij[taxa])
as.numeric(gammaPrior_Cont[[sam]]$kij[taxa])

#####
lc <- rgamma(length(u), shape = al_c, rate = be_c)
```

## Bayesian Idea

1) prior: $p$(parameter values) - $p(\theta)$

2) likelihood: $p$(data values | parameter values) - $p(D|\theta)$

3) posterior: $p$(parameter values | data values) - $p(\theta|D)$

## Bayesian Idea Cont.

$\begin{aligned}
p(D,\theta) &= p(\theta) \cdot p(D|\theta) \\
&= p(D) \cdot p(\theta|D) \\
p(D) \cdot p(\theta|D) &= p(\theta) \cdot p(D|\theta)\\
\end{aligned}$

\bigskip

### We can conclude that:

$\begin{aligned}
p(\theta|D) &= \frac{p(\theta) \cdot p(D|\theta)}{p(D)}\\
p(\theta|D) &\propto p(\theta) \cdot p(D|\theta)\\
(Shape\:of) posterior &= prior \cdot likelihood
\end{aligned}$

## 1) Grid approximation

1) We want to propose an interval that we use to evaluate the posterior with even-spaced points.

2) Compute prior and likelihood at each point

3) Compute the posterior

## Example 1)

Prior: $P(T)$ ~ $Normal(0.5,0.1)$ 

&nbsp

likelihood: $P(X$ ~ $Binomial(10,P(T))=8)$

```{R}
rangeP <- seq(0, 1, length.out = 100)
prior<- dnorm(x = rangeP, mean = .5, sd = .1)
likelihood <- dbinom(x = 8, prob = rangeP, size = 10)
posterior <- likelihood * prior
```

## Example 1) - Prior

```{R, echo=F}
plot(rangeP, prior/15,col = "red", type='l', main="Prior", ylab='density', xlab='')
```

## Example 1) - Likelihood

```{R, echo=F}
plot(rangeP, likelihood,type = "l", col="blue", main='likelihood',xlab='')
```

## Posterior

```{R, echo=F}
plot(rangeP, likelihood,type = "l", col='blue', xlab='', ylab='')
lines(rangeP, prior/15,col = "red")
lines(rangeP, posterior, col = "green")
legend("topleft", legend = c("Likelihood", "Prior", "Post"),
text.col = c('blue','red','green'), bty = "n")
```

## Graphs

```{R, echo=F}
library(purrr)
library(dplyr)
library(triangle)
library(ggplot2)
x <- 1; n <- 4
CoinsGrid <- 
  expand.grid(
    theta = seq(0, 1, by = 0.001)
  ) %>%
  mutate(
    prior = dtriangle(theta),        # triangle distribution
    likelihood  = map_dbl(theta, ~ dbinom(x = x, size = n, .x)),
    likelihood1 = likelihood / sum(likelihood) / 0.001, # "normalized"
    posterior0 = prior * likelihood,                    # unnormalized
    posterior = posterior0 / sum(posterior0) / 0.001    # normalized
  )

example <- data.frame(CoinsGrid)
```

```{R}
ggplot(example) + 
  geom_area(data=example,aes(x=theta,y=prior),alpha=0.2, fill="red", color="red")+
  geom_area(data=example,aes(x=theta,y=likelihood1),alpha=0.2,fill="blue", color="blue")+
  geom_area(data=example,aes(x=theta,y=posterior),alpha=0.2,fill="green", color="green")
```

## Grid Approximation - Limitation


```{R, echo=F}
grid_limit <- function(n1){
  set.seed(100)
  randomSample <- rnorm(100, trueMu, trueSig)
  # Grid approximation, mu %in% [0, 10] and sigma %in% [1, 3]
  grid <- expand.grid(mu = seq(0, 10, length.out = n1),
  sigma = seq(1, 3, length.out = n1))
  # Compute likelihood
  lik <- sapply(1:nrow(grid), function(x){
  sum(dnorm(x = randomSample, mean = grid$mu[x],
  sd = grid$sigma[x], log = T))
  })
  # Multiply (sum logs) likelihood and priors
  prod <- lik + dnorm(grid$mu, mean = 0, sd = 5, log = T) +
  dexp(grid$sigma, 1, log = T)
  # Standardize the lik x prior products to sum up to 1, recover unit
  prob <- exp(prod - max(prod))
  # Sample from posterior dist of mu and sigma, plot
  postSample <- sample(1:nrow(grid), size = 1e3, prob = prob)
  plot(grid$mu[postSample], grid$sigma[postSample],
  xlab = "Mu", ylab = "Sigma", pch = 16, col = rgb(0,0,0,.2))
  abline(v = trueMu, h = trueSig, col = "red", lty = 2)
}
```

Take a sample of n observations: 

Prior:

$\mu$ ~ $Normal(0,5)$

$\sigma$ ~ $Exp(1)$

Likelihood:

$X$ ~ $Normal(\mu,\sigma)$

by computing the likelihood for every $n \times n$ combination (grid approximation)

True$\mu$ = 5
True$\Sigma$ = 2

## Limitation

```{R}
trueMu <- 5
trueSig <- 2

grid_limit(100)

```

## Limitation

```{R}
grid_limit(50)

```

## Example

In a real scenario, we would compute some initial point estimates to choose an interval which should contain almost all of the probability mass of the posterior distribution.

## Example

\tiny

```{R, fig.width = 10, fig.height=5}
grid <- rgamma(n = 1000,shape = al_c, rate = be_c)
maxu <- max(grid)
minu <- min(grid)
u <- seq(minu, maxu, by = .1)


post_lr <- posterior(u,k,al_c,be_c)
#post_lr <- lapply(as.list(u), function(j){posterior(j, k, al_c, be_c )}) %>% unlist()


plot(u, post_lr)
```
\normalsize



## 2) Rejection Sampling

Sample data from a complicated distribution

- Target (distribution) function f(x) — The “difficult to sample from” distribution. Our distribution of interest!

- Proposal (distribution) function g(x) — The proxy distribution from which we can sample.

## 1)

![Target Function](target_function.png)

## 2)

![Proposal Function](proposal_function.png)

## 3)

![Constant](constant.png)

## 4)

Accept with probability $\frac{f(x)}{C\cdot g(x)}$
![ratio](ratio.png)

## Example

\tiny

blue is ratio
```{R}
library(AR)

simulation = AR.Sim( n = 200,
               f_X = function(y){dbeta(y,2.7,6.3)},
               Y.dist = "norm", Y.dist.par = c(0,1),
               Rej.Num = TRUE,
               Rej.Rate = TRUE,
               Acc.Rate = FALSE
)
simulation
```
\normalsize

## Output

n

The number/length of data which must be generated/simulated from $(f_X)$(TARGET) density.

Optimal c = 6.898

The numbers of Rejections = 1295

Ratio of Rejections = 0.866

## Example_DNA

\tiny

```{R}
lr_rs = AR.Sim( n = 20,
               f_X = function(y){(posterior(y,as.numeric(gammaPrior_Cont[[sam]]$kij[taxa]), al_c,be_c))},
               Y.dist = "gamma", Y.dist.par = c((k/mean(lc))*al_c,be_c), xlim=c(k-2*mean(lc),k),
               Rej.Num = TRUE,
               Rej.Rate = TRUE,
               Acc.Rate = FALSE
)
```

\normalsize

## Limitation

- Selecting the appropriate proposal function & finding its scaling constant

- Requires that the PDF of the target function is known

- Generally inefficient especially in higher dimensions


## 2.5) Adaptive Rejection Sampling

Define our proposal distribution in log space

^[Gilks, W. R., & Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. Journal of the Royal Statistical Society: Series C (Applied Statistics), 41(2), 337-348.]

## Example

sample 2000 values from the normal distribution N(2,3)


\tiny

```{R, fig.width = 10, fig.height=5}
library("ars")

f<-function(x,mu=0,sigma=1){-1/(2*sigma^2)*(x-mu)^2}
fprima<-function(x,mu=0,sigma=1){-1/sigma^2*(x-mu)}
mysample<-ars(2000,f,fprima,mu=2,sigma=3)
hist(mysample, breaks=30)

```
\normalsize

## Example 

\tiny

```{R, fig.size}

f1<-function(x,shape=al_c,scale=(1/be_c)){-1*((shape-1)*log(x)-x/scale)}
f1prima<-function(x,shape=al_c,scale=(1/be_c)){-1*((shape-1)/x-1/scale)}
mysample1<-ars(100,f1,f1prima,x=1,m=1,lb=TRUE,xlb=0,shape=al_c,scale=1/be_c, ub=TRUE, xub =max(k) )
hist(mysample1)
mean(mysample1)
```

\normalsize

## Monte Carlo

Relies on repeated random sampling to obtain numerical result

Ex) $\theta_t$ ~ Normal (0.5,$\sigma$)

## Monte Carlo Trace Plot

![Trace Plot](MonteCarlo_TracePlot.png)

## 50000 iteratons

![50000](50000.png)

## Markov property

Given the present, the future does not depend on the past. 

![markovproperty](markovproperty.png)

## Example 

Ex) $\theta_t$ ~ Normal ($\theta_{t-1},\sigma$)

Depends on the previous number on a sequence

## Trace Plot

![Trace Plot](MarkovChain_TracePlot.png)

## 50000 entries

![](50000mcmc.png)

## 3) Metropolis Hasting

The Metropolis–Hastings algorithm can draw samples from any probability distribution f(x), provided that we know a function q(x) proportional to the density of f and the values of q(x) can be calculated. The requirement that q(x) must only be proportional to the density

![mh](mh.png)

## Intuition

$\alpha = min\{\frac{f(b)}{f(a)},1\}$

![intuition](mkO7V.jpg)

## Limitation

- Dependence on starting value
  - Burn-in period
  
- Autocorrelation due to the Markov Chain properties

## Example

\tiny

```{R, fig.width = 10, fig.height=5}
chain <- MH_MCMC(itera = 100,
                            k = as.numeric(gammaPrior_Cont[[sam]]$kij[taxa]),
                            al_c = gammaPrior_Cont[[sam]]$alpha_ij_c[taxa],
                            be_c = gammaPrior_Cont[[sam]]$beta_ij_c[taxa],
                            startvalue_lamda_r = 0)

lr_star <- chain[50:100]
hist(lr_star, breaks=18)
```

\normalsize

## Gibbs Sampling

\tiny

![Gibbs](gs.png)
\normalsize

### Limitation

False Convergence

## Goodness of fit

\footnotesize

```{R}
goodness_of_fit <- function(lc,lr){
  kij_c_star <- rpois(length(lc),mean(lc))
  kij_r_star <- rpois(length(lc),mean(lr))
  kij_star <- kij_r_star+kij_c_star
  hist(kij_star, main=paste("Goodness of fit for",deparse(substitute(lr))) , breaks =30, xlab="counts")%>%
  abline(v=as.numeric(gammaPrior_Cont[[sam]]$kij[taxa]))
}

```

\normalsize

## Grid Approximation

```{R}

goodness_of_fit(lc,post_lr)

```

## Metropolis-Hasting

```{R}
goodness_of_fit(lc,lr_star)
```

## Rejection Sampling
```{R}
goodness_of_fit(lc,lr_rs)

```


