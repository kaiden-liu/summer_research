---
title: "Approximating posterior distribution and some samplers"
author: "Kaiden Liu"
date: "11/08/2021"
output:
  beamer_presentation:
    colortheme: "crane"
    fonttheme: "Default"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(warning = FALSE)
```

```{r, include=F}
library(BARBI)
library(tidyverse)
library(phyloseq)
library(DESeq2)
# library(R.utils)
# library(BiocParallel)
# library(doParallel)
# library(parallel)
library(HDInterval)
# library(grid)
# library(xtable)
# library(gtable)
# library(gridExtra)
# library(BiocStyle)
library(ggrepel)# library(ggrepel)
library(dplyr)
#R.utils::sourceDirectory("C:/Users/kaide/Documents/Summer_Research/week/week 11/R_BARBI_CJS")
```


```{r, include=F}
ps <- readRDS("C:/Users/kaide/Documents/Summer_Research/week/week 11/Data/ps_zymo.rds")
if(dim(otu_table(ps))[1]!=ntaxa(ps)){otu_table(ps) <- t(otu_table(ps))}
```



```{r, include=F}
ncont <- paste0("NegativeControl.",seq(1,10))
stan <- paste0("Standard.Dilution.1.",c(1,6,36,216,1296,7776,46656,279936))

sample_data(ps)$Name <- factor(sample_data(ps)$Name, levels = c(ncont,stan))

sample_names(ps) <- as.character(sample_data(ps)$Name)
```


```{r, include=F}
ASV <- as.character(paste0("ASV_",seq(1,ntaxa(ps))))
ASV.Genus <- paste0("ASV_",seq(1,ntaxa(ps)),"_",as.character(tax_table(ps)[,6]))
ASV.Genus.Species <- paste0(ASV,"_",as.character(tax_table(ps)[,6]),"_", as.character(tax_table(ps)[,7]))

df.ASV <- data.frame(seq.variant = taxa_names(ps), ASV = ASV, ASV.Genus = ASV.Genus, ASV.Genus.Species = ASV.Genus.Species)
```


```{r, include= F}
taxa_names(ps) <- df.ASV$ASV.Genus.Species
```




```{r adding_blocks, include=F}
blocks <- rep("Set1", nsamples(ps))

sample_data(ps)$block <- blocks
```


```{r filter_taxa, include=F}
ps <- prune_taxa(taxa_sums(ps) > 0, ps)
ps.standard <- subset_samples(ps, SampleType %in% c("Standard"))
prevTaxaP <- apply(otu_table(ps.standard), 1, function(x){sum(x>0)})

Contaminants1 <- names(prevTaxaP)[prevTaxaP == 0]
length(Contaminants1)
ps <- prune_taxa(prevTaxaP > 0, ps)
ps
```


```{r summary_stat, include=F}
table(sample_data(ps)$SampleType, sample_data(ps)$block)
colSums(otu_table(ps))
```


```{r list_of_phyloseq, include=F}
psBlockResult <- psBlockResults(ps, sampleTypeVar = "SampleType", caselevels = c("Standard"), controllevel="Negative", sampleName = "Name", blockVar = "block")

psByBlock <- psBlockResult[[1]]
psNCbyBlock <- psBlockResult[[2]] #contaminant intensity
psallzeroInNC <- psBlockResult[[3]] # zeros in negative control
psPlByBlock <- psBlockResult[[4]] # biological smaples

test<-lapply(psNCbyBlock, function(x) {
ps.to.dq = phyloseq_to_deseq2(x, design = ~1)
ps.to.dq = estimateSizeFactors(ps.to.dq,type="poscounts")
return(ps.to.dq)
}

)
test

```


```{r estimate_Cont_ncontrols, include=F}
alphaBetaNegControl <- alphaBetaNegControl(psNCbyBlock = psNCbyBlock, stringent = FALSE)
```

```{r estimate_Cont_plasma, include=F}
num_blks <- length(alphaBetaNegControl)
blks <- seq(1, num_blks) %>% as.list

gammaPrior_all_blks <- lapply(blks, function(x){
        gammaPrior <- alphaBetaContInPlasma(psPlByBlock = psPlByBlock, psallzeroInNC = psallzeroInNC, blk = x, alphaBetaNegControl = alphaBetaNegControl)
        return(gammaPrior)
})

```



```{r, include=F}
# First sample, second taxon
sam <- 1
taxa <- 2

# prior density for contamination intensity in biological samples
# we have only one batch
gammaPrior_Cont <- gammaPrior_all_blks[[1]]

# observed count in sam == 1 and second taxa
k <- as.numeric(gammaPrior_Cont[[sam]]$kij[taxa])
al_c <- gammaPrior_Cont[[sam]]$alpha_ij_c[taxa]
be_c <- gammaPrior_Cont[[sam]]$beta_ij_c[taxa]


# true intensity grid (alternatively we can choose min and max of k as well)
maxu <- max(rgamma(n = 1000,shape = al_c, rate = be_c))
# include minu
u <- seq(0, maxu, by = .1)

post_lr <- posterior(u,k,al_c,be_c)
# post_lr <- lapply(as.list(u), function(j){posterior(j, k, al_c, be_c )}) %>% unlist()

# plot(u, post_lr)
#######

lc <- rgamma(length(u), shape = al_c, rate = be_c)
# kij_c_lc <- lapply(as.list(lc), function(x){rpois(1, x)}) %>% unlist()
# 95% credible interval for lc
c(qgamma(.025, shape = al_c, rate = be_c), 
  qgamma(.925, shape = al_c, rate = be_c))


hist(lc)

#######

start.time<-Sys.time()
chain <- MH_MCMC(itera = 100,
                            k = as.numeric(gammaPrior_Cont[[sam]]$kij[taxa]),
                            al_c = gammaPrior_Cont[[sam]]$alpha_ij_c[taxa],
                            be_c = gammaPrior_Cont[[sam]]$beta_ij_c[taxa],
                            startvalue_lamda_r = 0)

end.time<-Sys.time()
time.taken_mh<- end.time-start.time

lr_star <- chain[50:100]
quantile(lr_star, c(0.025, 0.975))

hist(lr_star)

mean(lr_star)
mean(lc)

## generate the data 
kij_r_star <- rpois(1, mean(lr_star))
kij_c_star <- rpois(1, mean(lc))

kij_r_star+kij_c_star # is it closer to as.numeric(gammaPrior_Cont[[sam]]$kij[taxa])
as.numeric(gammaPrior_Cont[[sam]]$kij[taxa])

#####
lc <- rgamma(length(u), shape = al_c, rate = be_c)
```

## Bayes' theorem

1) prior: $p$(parameter values) - $p(\theta)$

2) likelihood: $p$(data values | parameter values) - $p(D|\theta)$

3) posterior: $p$(parameter values | data values) - $p(\theta|D)$

## Bayes' theorem Cont.

$\begin{aligned}
p(D,\theta) &= p(\theta) \cdot p(D|\theta) \\
&= p(D) \cdot p(\theta|D) \\
p(D) \cdot p(\theta|D) &= p(\theta) \cdot p(D|\theta)\\
\end{aligned}$

\bigskip

### We can conclude that:

$\begin{aligned}
p(\theta|D) &= \frac{p(\theta) \cdot p(D|\theta)}{p(D)}\\
p(\theta|D) &\propto p(\theta) \cdot p(D|\theta)\\
(Shape\:of) posterior &= prior \cdot likelihood
\end{aligned}$

## Grid approximation

1.	Define a discrete grid of possible $\theta$ values.

2.	Evaluate the prior pdf $f(\theta)$ and likelihood function $L(\theta|D)$ at each $\theta$ grid value.

3.	Obtain a discrete approximation of posterior pdf $f(\theta|D)$ by:

- calculating the product $f(\theta)$$L(\theta|D)$ at each $\theta$ grid value 

-  normalizing the products so that they sum to 1 across all $\theta$.

4.	Randomly sample n  $\theta$  grid values with respect to their corresponding normalized posterior probabilities.

## Example

![rainbow](rainbow.png)


\tiny

```{R, fig.width = 10, fig.height=5, include = F}

grid <- rgamma(n = 3000,shape = al_c, rate = be_c)
maxu <- max(grid)
minu <- min(grid)
u <- seq(minu, maxu, by = .1)


post_lr <- posterior(u,k,al_c,be_c)
#post_lr <- lapply(as.list(u), function(j){posterior(j, k, al_c, be_c )}) %>% unlist()


plot(u, post_lr)

lr_ga <- u[which.max(post_lr)]

```
\normalsize



## 2) Rejection Sampling

Sample data from a complicated distribution

- Target (distribution) function f(x) — The “difficult to sample from” distribution. Our distribution of interest!

- Proposal (distribution) function g(x) — The proxy distribution from which we can sample.

## 1)

![Target Function](target_function.png)

## 2)

![Proposal Function](proposal_function.png)

## 3)

![Constant](constant.png)

## 4)

Accept with probability $\frac{f(x)}{C\cdot g(x)}$
![ratio](ratio.png)

## Example

\tiny

blue is ratio
```{R}
library(AR)

simulation = AR.Sim( n = 200,
               f_X = function(y){dbeta(y,2.7,6.3)},
               Y.dist = "norm", Y.dist.par = c(0,1),
               Rej.Num = TRUE,
               Rej.Rate = TRUE,
               Acc.Rate = FALSE
)
simulation
```
\normalsize

## Output

n

The number/length of data which must be generated/simulated from $(f_X)$(TARGET) density.

Optimal c = 6.898

The numbers of Rejections = 1295

Ratio of Rejections = 0.866

## Example_DNA

\tiny

```{R}

start.time<-Sys.time()
lr_rs = AR.Sim( n = 20,
               f_X = function(y){(posterior(y,as.numeric(gammaPrior_Cont[[sam]]$kij[taxa]), al_c,be_c))},
               Y.dist = "gamma", Y.dist.par = c((k/mean(lc))*al_c,be_c), xlim=c(k-2*mean(lc),k),
               Rej.Num = TRUE,
               Rej.Rate = TRUE,
               Acc.Rate = FALSE
)
end.time<-Sys.time()
time.taken_rs<- end.time-start.time
```

\normalsize

## Limitation

- Selecting the appropriate proposal function & finding its scaling constant

- Requires that the PDF of the target function is known

- Generally inefficient especially in higher dimensions


## 2.5) Adaptive Rejection Sampling

Define our proposal distribution in log space

^[Gilks, W. R., & Wild, P. (1992). Adaptive rejection sampling for Gibbs sampling. Journal of the Royal Statistical Society: Series C (Applied Statistics), 41(2), 337-348.]

## Example

sample 2000 values from the normal distribution N(2,3)


\tiny

```{R, fig.width = 10, fig.height=5}
library("ars")

f<-function(x,mu=0,sigma=1){-1/(2*sigma^2)*(x-mu)^2}
fprima<-function(x,mu=0,sigma=1){-1/sigma^2*(x-mu)}
mysample<-ars(2000,f,fprima,mu=2,sigma=3)
hist(mysample, breaks=30)

```
\normalsize



## Monte Carlo

Relies on repeated random sampling to obtain numerical result

Ex) $\theta_t$ ~ Normal (0.5,$\sigma$)

## Monte Carlo Trace Plot

![Trace Plot](MonteCarlo_TracePlot.png)

## 50000 iteratons

![50000](50000.png)

## Markov property

Given the present, the future does not depend on the past. 

![markovproperty](markovproperty.png)

## Example 

Ex) $\theta_t$ ~ Normal ($\theta_{t-1},\sigma$)

Depends on the previous number on a sequence

## Trace Plot

![Trace Plot](MarkovChain_TracePlot.png)

## 50000 entries

![](50000mcmc.png)

## 3) Metropolis Hasting

The Metropolis–Hastings algorithm can draw samples from any probability distribution f(x), provided that we know a function q(x) proportional to the density of f and the values of q(x) can be calculated. The requirement that q(x) must only be proportional to the density

![mh](mh.png)

## Intuition

$\alpha = min\{\frac{f(b)}{f(a)},1\}$

```{R}
x= seq(-3,3, by=0.1)
y=dnorm(x)
plot(x,2*y, type="l", col="blue")
lines(x,y, col="red")
legend("topleft", legend = c("Proposal", "Target"),
text.col = c('blue','red'), bty = "n")

```

## Limitation

- Dependence on starting value
  - Burn-in period
  
- Autocorrelation due to the Markov Chain properties

## Example

\tiny

```{R, fig.width = 10, fig.height=5}
start.time<-Sys.time()
chain <- MH_MCMC(itera = 600,
                            k = as.numeric(gammaPrior_Cont[[sam]]$kij[taxa]),
                            al_c = gammaPrior_Cont[[sam]]$alpha_ij_c[taxa],
                            be_c = gammaPrior_Cont[[sam]]$beta_ij_c[taxa],
                            startvalue_lamda_r = 0)
end.time<-Sys.time()
time.taken_mh <- end.time-start.time

lr_star <- chain[50:length(chain)]
hist(lr_star, breaks=30)
```

\normalsize

## Gibbs Sampling

\tiny

![Gibbs](gs.png)

\normalsize

## Limitation

|   | 0   | 1   |
|---|-----|-----|
| 1 | 0   | $\frac{1}{2}$|
| 0 | $\frac{1}{2}$ | 0   |
  
  
## Limitation

![Limitation](gibbs_challenge.png)

## Example

```{R, echo=F}
x = matrix(-5,2000); y = matrix(-5,2000) #initial value = (-5,-5) =(x[j],y[j])
for(i in 2:2000){
	#sample from x |y
	#generate one sample from a uniform distribution
	u=runif(1,min=0,max=1)
	#using CDF method
	x[i]=sqrt(u*(6*y[i-1]+8)+(1.5*y[i-1]+1)*(1.5*y[i-1]+1))-(1.5*y[i-1]+1)
	#sample from y | x
	u=runif(1,min=0,max=1)
	y[i]=sqrt((2*u*(4*x[i]+10))/3+((2*x[i]+2)/3)*((2*x[i]+2)/3))-((2*x[i]+2)/3)
}

a<-1:50
plot(x[a],y[a],type = "b")
```

## Example
```{R, message=F}
#library(plotly)
#plot_ly(x=den$x, y=den$y, z=den$z) %>% add_surface

```

## Goodness of fit

\tiny

```{R}
goodness_of_fit <- function(lc,lr){
  kij_c_star <- rpois(length(lc),mean(lc))
  kij_r_star <- rpois(length(lc),mean(lr))
  kij_star <- kij_r_star+kij_c_star
  hist(kij_star, 
       main=paste("Goodness of fit for",deparse(substitute(lr))) , breaks =30, xlab="counts")
  abline(v=as.numeric(gammaPrior_Cont[[sam]]$kij[taxa]))
}

```

\normalsize

## Grid Approximation

```{R}
#goodness_of_fit(lc,post_lr)

# actual observed counts
k

# point estimation

mean(lc) + lr_ga

```

## Metropolis-Hasting

```{R}
goodness_of_fit(lc,lr_star)

time.taken_mh

```

## Rejection Sampling
```{R}
goodness_of_fit(lc,lr_rs)

time.taken_rs

```


